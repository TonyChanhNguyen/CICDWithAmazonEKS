[
{
	"uri": "/2-prerequiste/2.1-createcloud9workspace/",
	"title": "Create Cloud9 workspace",
	"tags": [],
	"description": "",
	"content": "Create Cloud9 workspace   Go to Cloud9 at region ap-southeast-1.\n  Click on Create environment.   At Create environment page, input FCJ-Workspace at Name field.\n  Input Workspace for hands on workshop at Description field.\n  At Environment type field, keep default New EC2 instance.\n  At Instance type field, select Additional instance types.\n  At Additional instance types field, select t3.large.   Scroll down to the end of page and click on Create.   The workspace instance is being created.   It will take you about 2 minutes for the instance is created successfully.\n  After the instance is created successfully, click on Open to start your workspace.   "
},
{
	"uri": "/3-eksstoragewithebs/3.1-installcsidriver/",
	"title": "Install Amazon EBS CSI Driver",
	"tags": [],
	"description": "",
	"content": "The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver manages the lifecycle of Amazon EBS volumes as storage for the Kubernetes Volumes that you create. The Amazon EBS CSI driver makes Amazon EBS volumes for these types of Kubernetes volumes: generic ephemeral volumes and persistent volumes.\nCreate IAM Policy The Amazon EBS CSI plugin requires IAM permissions to make calls to AWS APIs on your behalf.\n  Go to IAM.\n  Select Policies section.\n  Click on Create policy.   At Policy editor field, select JSON tab.\n  Then, paste the below JSON.\n  {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:CreateSnapshot\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateVolume\u0026#34;,\r\u0026#34;ec2:DeleteSnapshot\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteVolume\u0026#34;,\r\u0026#34;ec2:DescribeInstances\u0026#34;,\r\u0026#34;ec2:DescribeSnapshots\u0026#34;,\r\u0026#34;ec2:DescribeTags\u0026#34;,\r\u0026#34;ec2:DescribeVolumes\u0026#34;,\r\u0026#34;ec2:DetachVolume\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r}  Then, click on Next.   Input Amazon_EBS_CSI_Driver as Policy name.\n  Input Policy for EC2 Instances to access Elastic Block Store as Description - optional.\n  Click on Create policy.   Get the IAM role Worker Nodes using  At Cloud9 Terminal, using below command to find out IAM role name that Worker Nodes using.  kubectl -n kube-system describe configmap aws-auth Verify the value of rolearn to find out the IAM Role name. Example: In this case, the IAM Role name is eksctl-fcj-storage-cluster-nodegro-NodeInstanceRole-dN18fcwv9euu.\nAssociate policy to IAM Role   Go to IAM Role.\n  At Search feature, input the IAM Role name you had found out.\n  Click on it.   Click on Attach policies of Add permissions feature.   Search the policy name Amazon_EBS_CSI_Driver.\n  Select the result.\n  Click on Add permissions.   Deploy Amazon EBS CSI Driver  At Cloud9 Terminal, execute below command.  kubectl apply -k \u0026#34;github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\u0026#34; Verify the result. Make sure that there are ebs-csi pods are running at namespace kube-system.  kubectl get pods -n kube-system You had installed Amazon EBS CSI Driver successfully, move to next step to demonstrate how to use it to deploy EBS Storage for Amazon EKS Cluster. "
},
{
	"uri": "/4-eksstoragewithefs/4.1-installecsidriver/",
	"title": "Install Amazon EFS CSI Driver",
	"tags": [],
	"description": "",
	"content": "Create IAM Role The Amazon EFS CSI driver requires IAM permissions to interact with your file system. Create an IAM role and attach the required AWS managed policy to it.\n At Cloud9 Terminal, run the following commands to create the IAM role.  export cluster_name=fcj-storage-cluster\rexport role_name=AmazonEKS_EFS_CSI_DriverRole\reksctl create iamserviceaccount \\\r--name efs-csi-controller-sa \\\r--namespace kube-system \\\r--cluster $cluster_name \\\r--region ap-southeast-1 \\\r--role-name $role_name \\\r--attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEFSCSIDriverPolicy \\\r--override-existing-serviceaccounts \\\r--approve\rTRUST_POLICY=$(aws iam get-role --role-name $role_name --query \u0026#39;Role.AssumeRolePolicyDocument\u0026#39; | \\\rsed -e \u0026#39;s/efs-csi-controller-sa/efs-csi-*/\u0026#39; -e \u0026#39;s/StringEquals/StringLike/\u0026#39;)\raws iam update-assume-role-policy --role-name $role_name --policy-document \u0026#34;$TRUST_POLICY\u0026#34; Verify created Service Account.\nExecute below command to verify the Service Account name efs-csi-controller-sa in kube-system namespace.  kubectl get sa efs-csi-controller-sa -n kube-system Verify the created IAM Role.\n Go to IAM Role.\n  Input the IAM Role Name AmazonEKS_EFS_CSI_DriverRole at Search feature.\n  Click on IAM Role AmazonEKS_EFS_CSI_DriverRole.   There is a policy named AmazonEFSCSIDriverPolicy attached to IAM Role.   Navigate to Trust replationships tab.\n  Verify that there is a condition with value is system:serviceaccount:kube-system:efs-csi-.   Install the Amazon EFS CSI driver  Get the version of cluster.  kubectl version The version of cluster is Server Version (1.29 not 1.29.3). 2. View the names of add-ons available for a cluster version.\neksctl utils describe-addon-versions --region=ap-southeast-1 --kubernetes-version 1.29 | grep AddonName Determine the current add-ons and add-on versions installed on your cluster  eksctl get addon --cluster fcj-storage-cluster --region ap-southeast-1 Get Service Account Role ARN of efs-csi-controller-sa service account.  kubectl describe sa efs-csi-controller-sa -n kube-system Save the value of eks.amazonaws.com/role-arn at Annotations field.  Create an Amazon EKS add-on. Replace with specific value on command eksctl create addon \u0026ndash;cluster \u0026lt;REPLACE_CLUSTER_NAME\u0026gt; \u0026ndash;name \u0026lt;REPLACE_ADDON_NAME\u0026gt; \u0026ndash;region \u0026lt;REPLACE_REGION_ID\u0026gt; \u0026ndash;version latest \u0026ndash;service-account-role-arn \u0026lt;REPLACE_IAM_ROLE_ARN\u0026gt; \u0026ndash;force. Then execute the command.  eksctl create addon --cluster fcj-storage-cluster --name aws-efs-csi-driver --region ap-southeast-1 --version latest \\\r--service-account-role-arn arn:aws:iam::170074558790:role/AmazonEKS_EFS_CSI_DriverRole --force Determine the current add-ons and add-on versions installed on your cluster again.  eksctl get addon --cluster fcj-storage-cluster --region ap-southeast-1 "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Before we dive into the implementation, below is a summary of the two AWS storage services we\u0026rsquo;ll utilize and integrate with EKS:\n Amazon Elastic Block Store (supports EC2 only): a block storage service that provides direct access from EC2 instances and containers to a dedicated storage volume designed for both throughput and transaction-intensive workloads at any scale. Amazon Elastic File System (supports Fargate and EC2): a fully managed, scalable, and elastic file system well suited for big data analytics, web serving and content management, application development and testing, media and entertainment workflows, database backups, and container storage. EFS stores your data redundantly across multiple Availability Zones (AZ) and offers low latency access from Kubernetes pods irrespective of the AZ in which they are running.  Two type services below are not in scope of this workshop.\n Amazon FSx for NetApp ONTAP (supports EC2 only): Fully managed shared storage built on NetAppâ€™s popular ONTAP file system. FSx for NetApp ONTAP stores your data redundantly across multiple Availability Zones (AZ) and offers low latency access from Kubernetes pods irrespective of the AZ in which they are running. FSx for Lustre (supports EC2 only): a fully managed, high-performance file system optimized for workloads such as machine learning, high-performance computing, video processing, financial modeling, electronic design automation, and analytics. With FSx for Lustre, you can quickly create a high-performance file system linked to your S3 data repository and transparently access S3 objects as files. Amazon Simple Storage Service (supports EC2 only): is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize data, and configure fine-tuned access controls to meet specific business, organizational, and compliance requirements.  It\u0026rsquo;s also very important to be familiar with some concepts about Kubernetes Storage:\n Volumes: On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem is the loss of files when a container crashes. The kubelet restarts the container but with a clean state. A second problem occurs when sharing files between containers running together in a Pod. The Kubernetes volume abstraction solves both of these problems. Familiarity with Pods is suggested. Ephemeral Volumes are designed for these use cases. Because volumes follow the Pod\u0026rsquo;s lifetime and get created and deleted along with the Pod, Pods can be stopped and restarted without being limited to where some persistent volume is available. Persistent Volumes (PV) is a piece of storage in a cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It\u0026rsquo;s a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system. Persistent Volume Claim (PVC) is a request for storage by a user. It\u0026rsquo;s similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany. Storage Classes provides a way for administrators to describe the \u0026ldquo;classes\u0026rdquo; of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by cluster administrators. Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called \u0026ldquo;profiles\u0026rdquo; in other storage systems. Dynamic Volume Provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when it is requested by users.  In this workshop, we will only focus on how to integrate persistent storage on EKS Cluster with Amazon EBS and Amazon EFS by Static Provisioning and Dynamic Provisioning.\n"
},
{
	"uri": "/",
	"title": "Persistent Storage on Amazon EKS",
	"tags": [],
	"description": "",
	"content": "Persistent Storage on Amazon EKS Overview This workshop will provide a high level overview on how to integrate two AWS Storage services as Persistent Storage with your EKS cluster (Amazon ABS and Amazon EFS).\nContent  Introduction Prerequisites Amazon EKS Storage with Amazon EBS Amazon EKS Storage with Amazon EFS Cleanup resource  "
},
{
	"uri": "/4-eksstoragewithefs/4.2-createefs/",
	"title": "Create EFS File System",
	"tags": [],
	"description": "",
	"content": "Amazon EFS CSI driver supports dynamic provisioning and static provisioning. Currently, Dynamic Provisioning creates an access point for each PV. This mean an Amazon EFS file system has to be created manually on AWS first and should be provided as an input to the storage class parameter. For static provisioning, the Amazon EFS file system needs to be created manually on AWS first. After that, it can be mounted inside a container as a volume using the driver.\nCreate Security Group  Retrieve the VPC ID that your cluster is in and store it in a variable for use in a later step  vpc_id=$(aws eks describe-cluster \\\r--name fcj-storage-cluster \\\r--query \u0026#34;cluster.resourcesVpcConfig.vpcId\u0026#34; \\\r--output text)\recho $vpc_id Retrieve the CIDR range for your cluster\u0026rsquo;s VPC and store it in a variable for use in a later step  cidr_range=$(aws ec2 describe-vpcs \\\r--vpc-ids $vpc_id \\\r--query \u0026#34;Vpcs[].CidrBlock\u0026#34; \\\r--output text \\\r--region ap-southeast-1)\recho $cidr_range Now we will create a security group with an inbound rule that allows inbound NFS traffic for your Amazon EFS mount points.\nCreate a security group  security_group_id=$(aws ec2 create-security-group \\\r--group-name MyEfsSecurityGroup \\\r--description \u0026#34;My EFS security group\u0026#34; \\\r--vpc-id $vpc_id \\\r--output text) Create an inbound rule that allows inbound NFS traffic from the CIDR for your cluster\u0026rsquo;s VPC.  aws ec2 authorize-security-group-ingress \\\r--group-id $security_group_id \\\r--protocol tcp \\\r--port 2049 \\\r--cidr $cidr_range Create EFS File System  Create a file system.  file_system_id=$(aws efs create-file-system \\\r--region ap-southeast-1 \\\r--performance-mode generalPurpose \\\r--query \u0026#39;FileSystemId\u0026#39; \\\r--output text)\recho $file_system_id Go to EFS to verify.  There is a created file system with File system ID match with value of file_system_id.\nClick on the EFS file system.   Now we will create a mount target 4. Navigate to Network tab. There is no Mount Target created.\nDetermine the IP address of your cluster nodes.  kubectl get nodes In my case, the private IP address of cluster node is 192.168.18.22.\nDetermine the IDs of the subnets in your VPC and which Availability Zone the subnet is in.  aws ec2 describe-subnets \\\r--region ap-southeast-1 \\\r--filters \u0026#34;Name=vpc-id,Values=$vpc_id\u0026#34; \\\r--query \u0026#39;Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}\u0026#39; \\\r--output table Cluster node with IP is 192.168.18.22 belongs to CIDR Block 192.168.0.0/19 and SubnetId subnet-0a75ff93096b88a38.\nAdd mount targets for the subnets that your nodes are in. Replace with your subnet-id.  aws efs create-mount-target \\\r--file-system-id $file_system_id \\\r--subnet-id subnet-0a75ff93096b88a38 \\\r--security-groups $security_group_id Go to Network tab of EFS to verify the mount target again.   There is a new created target mount appeared.\nYou had create a EFS File System and its Mount Target successfully. "
},
{
	"uri": "/2-prerequiste/2.2-modifyiamrole/",
	"title": "Modify IAM role",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a IAM Role and assign it to workspace instance.\nCreate IAM role   Click IAM to navigate to IAM service.\n  Click on Role.\n  Click on Create role.   At Trusted entity type field, select AWS service.\n  At Service or use case field, select EC2.   Then, click on Next.   At Permissions policies field, select policy name AdministratorAccess.   Then, click on Next.   At Name, review, and create page, input eksworkspace-administrator at Role name field.   Then, scroll down to the end of page and click on Create role.   Assign role to workspace instance   At AWS Cloud9 interface, click on Manage EC2 instance.   You will see the created workspace instance. Then, click to select it.\n  Click on Action.\n  Click on Security.\n  Click on Modify IAM role.   Select the role name eksworkspace-administrator which was created at above steps.\n  Then, click on Update IAM role.   New IAM role was updated successfully.   Update Cloud9 configuration Cloud9 will manage IAM credentials automatically. This default configuration is currently not compatible with EKS authentication via IAM, we will need to disable this feature and use the IAM Role.\n\r  At AWS Cloud9 interface, click on AWS Cloud9.\n  Select Preferences.   At AWS Settings, disable AWS managed temporary credentials.   To ensure that temporary credentials are not saved in Cloud9, we will delete all existing credentials with the command below.\n  rm -vf ${HOME}/.aws/credentials "
},
{
	"uri": "/2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Overview To conduct the lab, we have to prepare the Cloud9 workspace instance and create the IAM role for the Cloud9 instance.\nContent  2.1 Create Cloud9 workspace 2.2 Modify IAM Role 2.3 Installation 2.4 Create Amazon EKS Cluster  "
},
{
	"uri": "/3-eksstoragewithebs/3.2-staticprovision/",
	"title": "Static Provisioning",
	"tags": [],
	"description": "",
	"content": "In this step, we will practice how to create and consume a PersistentVolume from an existing EBS volume with static provisioning.\nVerify the Node\u0026rsquo;s Availability Zone First, we need to check which Availability Zone that our EC2 Managed Node Group Instance is placing.\n Go to EC2 Instances. Verify its Availability Zone.   In my case the Availability Zone of Node is ap-southeast-1b, so now i will create an EBS Volume at ap-southeast-1b.\nCreate an EBS volume  Go to EBS Volumes. You will see there are two volumes: default volume of Cloud9 instance and default volume of NodeGroup instance.   Click on Create volume to create new volume.   Replace to 20 at Size (GiB) field.\n  Set Availability Zone as Node\u0026rsquo;s Availability Zone. In this case is ap-southeast-1b.   Scroll down to the end of page and click on Create volume.   Edit Name of created EBS Volume fcj-ebs-strorage-eks.   Save the Volume ID to use later.   Create manifest files  At Cloud9 Terminal, create a directory name static-provision/kube-manifest.  mkdir -p static-provision/kube-manifest\rls\rls static-provision Create a file named pv.yaml inside static-provision/kube-manifest.  touch static-provision/kube-manifest/pv.yaml Open pv.yaml file, paste the below code.  apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: test-pv\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity:\rstorage: 10Gi\rcsi:\rdriver: ebs.csi.aws.com\rfsType: ext4\rvolumeHandle: \u0026lt;REPLACE-WITH-YOUR-VOLUME-ID\u0026gt;  Replace with your Volume ID at volumeHandle. Then save it.   Create a file named pv-claim.yaml inside static-provision/kube-manifest.\n  touch static-provision/kube-manifest/pv-claim.yaml Open pv-claim.yaml file, paste the below code. Then save it.  apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: ebs-claim\rspec:\rstorageClassName: \u0026#34;\u0026#34; # Empty string must be explicitly set otherwise default StorageClass will be set\rvolumeName: test-pv\raccessModes:\r- ReadWriteOnce\rresources:\rrequests:\rstorage: 10Gi Create a file named pod.yaml inside static-provision/kube-manifest.  touch static-provision/kube-manifest/pod.yaml Open pod.yaml file, paste the below code. Then save it.  apiVersion: v1\rkind: Pod\rmetadata:\rname: app\rspec:\rcontainers:\r- name: app\rimage: centos\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;]\rvolumeMounts:\r- name: persistent-storage\rmountPath: /data\rvolumes:\r- name: persistent-storage\rpersistentVolumeClaim:\rclaimName: ebs-claim Deploy the resources  At Cloud9 Terminal, execute the below command.  kubectl apply -f static-provision/kube-manifest Verify the result  List the Persistent Volume (PV).  kubectl get pv There is a PV named test-pv with capacity is 10GB.\nList the Persistent Volume Claim (PVC).  kubectl get pvc There is a PVC named ebs-claim, status is BOUND, volume is test-pv and capacity is 10GB.\nList the Pod.  kubectl get pod There is a Pod named app with status is Running.\nWe will validate the pod successfully wrote data to the statically provisioned volume.  kubectl exec app -- cat /data/out.txt There are a lot of data wrote to /data/out.txt file. Let take note the content of three first data to compare later. In my case is Mon May 6 17:15:04 UTC 2024, Mon May 6 17:15:09 UTC 2024 and Mon May 6 17:15:14 UTC 2024.\nNow, we will delete the pod then create another new pod to verify data in EBS volume is persistent with the new one.\nDelete the current Pod.  kubectl delete pod app 6. List all Pods again to make sure it was deleted.\nkubectl get pod Create a new Pod.  kubectl apply -f static-provision/kube-manifest List created Pod.  kubectl get pod Verify the data on /data/out.txt again.  kubectl exec app -- cat /data/out.txt As you can see, the three first of data on /data/out.txt now are still Mon May 6 17:15:04 UTC 2024, Mon May 6 17:15:09 UTC 2024 and Mon May 6 17:15:14 UTC 2024. The data is still kept while the pod is replaced with another one.\nClean up  Execute the below command to delete all created resources.  kubectl delete -f static-provision/kube-manifest Verify that all resources had been deleted successfully.  kubectl get pv,pvc,pod All of them are deleted.\n Go to EBS Volume to delete the EBS Volume named fcj-ebs-strorage-eks.\n  Select the volume named fcj-ebs-strorage-eks.\n  Click on Actions.\n  Click on Delete volume   Input delete to confirm.\n  Then, click on Delete.   "
},
{
	"uri": "/3-eksstoragewithebs/",
	"title": "Amazon EKS Persistent Storage with Amazon EBS",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Elastic Block Store is an easy-to-use, scalable, high-performance block-storage service. It provides persistent volume (non-volatile storage) to users. Persistent storage enables users to store their data until they decide to delete the data.\nContent  3.1 Install EBS CSI Driver 3.2 Static Provisioning 3.3 Dynamic Provisioning  "
},
{
	"uri": "/3-eksstoragewithebs/3.3-dynamicprovision/",
	"title": "Dynamic Provisioning",
	"tags": [],
	"description": "",
	"content": "Create manifest files  At Cloud9 Terminal, create a directory name dynamic-provision/kube-manifest.  mkdir -p dynamic-provision/kube-manifest Create a file named storageclass.yaml inside dynamic-provision/kube-manifest.  touch dynamic-provision/kube-manifest/storageclass.yaml Open storageclass.yaml file, paste the below code. Then save it.  apiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: ebs-sc\rprovisioner: ebs.csi.aws.com\rvolumeBindingMode: WaitForFirstConsumer Create a file named pv-claim.yaml inside dynamic-provision/kube-manifest.  touch dynamic-provision/kube-manifest/pv-claim.yaml Open pv-claim.yaml file, paste the below code. Then save it.  apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: ebs-claim\rspec:\raccessModes:\r- ReadWriteOnce\rstorageClassName: ebs-sc\rresources:\rrequests:\rstorage: 10Gi Create a file named pod.yaml inside dynamic-provision/kube-manifest.  touch dynamic-provision/kube-manifest/pod.yaml Open pod.yaml file, paste the below code. Then save it.  apiVersion: v1\rkind: Pod\rmetadata:\rname: app\rspec:\rcontainers:\r- name: app\rimage: centos\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;]\rvolumeMounts:\r- name: persistent-storage\rmountPath: /data\rvolumes:\r- name: persistent-storage\rpersistentVolumeClaim:\rclaimName: ebs-claim Deploy resources  At Cloud9 Terminal, execute the below command.  kubectl apply -f dynamic-provision/kube-manifest Verify the result  List all created resources.  kubectl get sc,pvc,pod There is a Pod named app with status is Running, a Persistent Volume Claim named ebs-claim with status is Bound and capacity is 10Gi and a Storage Class named ebs-sc with Provider is ebs.csi.aws.com. Storage Class will dynamically create an EBS Volume with volume size defined at spec.resources.requests.storage (10Gi)..\nGo to EBS Volume to verify the result.   There is a new EBS Volume with Availability Zone same as Node\u0026rsquo;s AZ.\nWe will validate the pod successfully wrote data to the dynamically provisioned volume.  kubectl exec app -- cat /data/out.txt There are a lot of data wrote to /data/out.txt file. Let take note the name of three first data to compare later. In my case is Mon May 6 19:58:43 UTC 2024, Mon May 6 19:58:48 UTC 2024 and Mon May 6 19:58:53 UTC 2024.\nNow, we will delete the pod then create another new pod to verify data in EBS volume is persistent with the new one.\nDelete the current Pod app.  kubectl delete pod app 5. List all Pods again to make sure it was deleted.\nkubectl get pod 6. Create a new Pod.\nkubectl apply -f dynamic-provision/kube-manifest 7. List created Pod.\nkubectl get pod 8. Verify the data on /data/out.txt again.\nkubectl exec app -- cat /data/out.txt As you can see, the three first of data on /data/out.txt now are still Mon May 6 19:58:43 UTC 2024, Mon May 6 19:58:48 UTC 2024 and Mon May 6 19:58:53 UTC 2024. The data is still kept while the pod is replaced with another one.\nClean up  Execute the below command to delete all created resources.  kubectl delete -f dynamic-provision/kube-manifest Verify that all resources had been deleted successfully.  kubectl get sc,pvc,pod All of them are deleted. Because Amazon EBS Volume is created dynamically by Storage Class. So when Storage Class is deleted, Amazon EBS Volume is also removed.\nGo to EBS Volume to verify the result.   Amazon EBS Volume was removed automatically.\n"
},
{
	"uri": "/2-prerequiste/2.3-installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will install necessary tools: awscli, kubectl and eksctl.\nUpgrade awscli  Copy and paste the command below into Terminal of Cloud9 Workspace to upgrade awscli.  sudo pip install --upgrade awscli \u0026amp;\u0026amp; hash -r Install kubectl  At Cloud9 Terminal, execute those command to install kubectl.   Update the instance packages.  sudo yum update  Install kubectl.  curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check version of kubectl.  kubectl version --client Install eksctl  At Cloud9 terminal, execute those command to install eksctl.   Download and extract the latest release of eksctl with the following command.  curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  Move the extracted binary to /usr/local/bin.  sudo mv /tmp/eksctl /usr/local/bin  Test that your installation was successful with the following command  eksctl version "
},
{
	"uri": "/4-eksstoragewithefs/4.3-staticprovision/",
	"title": "Static Provisioning",
	"tags": [],
	"description": "",
	"content": "Store existing resources.  Create a dicrectory EBS to store all resources of EBS section.  mkdir EBS Move all directories dynamic-provision and static-provision to EBS directory.  mv dynamic-provision EBS\rmv static-provision EBS Create Manifest files.  Create a directory name EFS to store the kube-manifest files.  mkdir EFS Create the directory for Static Provisioning section.  mkdir -p EFS/static-provision/kube-manifest\rls EFS/static-provision Create file name storageclass.yaml inside static-provision/kube-manifest.  touch EFS/static-provision/kube-manifest/storageclass.yaml Open storageclass.yaml file, paste the below code.  apiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: efs-sc\rprovisioner: efs.csi.aws.com Create file name pv.yaml inside static-provision/kube-manifest.  touch EFS/static-provision/kube-manifest/pv.yaml Open pv.yaml file, paste the below code. Input your EFS file system ID at volumeHandle.  apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: efs-pv\rspec:\rcapacity:\rstorage: 5Gi\rvolumeMode: Filesystem\raccessModes:\r- ReadWriteOnce\rstorageClassName: efs-sc\rpersistentVolumeReclaimPolicy: Retain\rcsi:\rdriver: efs.csi.aws.com\rvolumeHandle: fs-0db5cdf65fc6c821d Create file name pvc.yaml inside static-provision/kube-manifest.  touch EFS/static-provision/kube-manifest/pvc.yaml Open pvc.yaml file, paste the below code.  apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: efs-claim\rspec:\raccessModes:\r- ReadWriteOnce\rstorageClassName: efs-sc\rresources:\rrequests:\rstorage: 5Gi Create file name pod.yaml inside static-provision/kube-manifest.  touch EFS/static-provision/kube-manifest/pod.yaml Open pod.yaml file, paste the below code.  apiVersion: v1\rkind: Pod\rmetadata:\rname: efs-app\rspec:\rcontainers:\r- name: app\rimage: centos\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out.txt; sleep 5; done\u0026#34;]\rvolumeMounts:\r- name: persistent-storage\rmountPath: /data\rvolumes:\r- name: persistent-storage\rpersistentVolumeClaim:\rclaimName: efs-claim Deploy resources  Execute below command to deploy the resources.  kubectl apply -f EFS/static-provision/kube-manifest List all created resources.  kubectl get sc,pv,pvc,pod STATUS of Persistent Volume (pv) and Persistent Volume Claim (pvc) must be Bound, and of Pod must be Running.\n\rVerify the result.  Execute below command to verify that data is written onto EFS filesystem.  kubectl exec -ti efs-app -- head /data/out.txt There are a lot of data wrote to /data/out.txt file. Let take note the name of three first data to compare later. In my case is Wed May 8 11:31:17 UTC 2024, Wed May 8 11:31:22 UTC 2024 and Wed May 8 11:31:27 UTC 2024.\nNow we will delete the current Pod. Then list all resources again to make sure that Pod was deleted.  kubectl delete pod efs-app\rkubectl get sc,pv,pvc,pod Pod efs-app was deleted.\nCreate a new pod.  kubectl apply -f EFS/static-provision/kube-manifest/pod.yaml\rkubectl get pod 4. Verify the data on /data/out.txt again.\nkubectl exec -ti efs-app -- head /data/out.txt As you can see, the three first of data on /data/out.txt now are still Wed May 8 11:31:17 UTC 2024, Wed May 8 11:31:22 UTC 2024 and Wed May 8 11:31:27 UTC 2024. The data is still kept while the pod is replaced with another one.\nClean up  Execute below command to delete all created resources.  kubectl delete -f EFS/static-provision/kube-manifest Verify that all resources had been deleted successfully.  kubectl get sc,pv,pvc,pod All of them are deleted.\n"
},
{
	"uri": "/4-eksstoragewithefs/",
	"title": "Amazon EKS Persistent Storage with Amazon EFS",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Elastic File System is a simple, serverless, set-and-forget elastic file system for use with AWS Cloud services and on-premises resources. It\u0026rsquo;s built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Content  4.1 Install EFS CSI Driver 4.2 Create EFS File System 4.3 Static Provisioning 4.4 Dynamic Provisioning  "
},
{
	"uri": "/2-prerequiste/2.4-createekscluster/",
	"title": "Create Amazon EKS Cluster",
	"tags": [],
	"description": "",
	"content": "In previous step, we installed necessary tools: awscli, kubectl and eksctl. Now, we will process to create an Amazon EKS Cluster with managed Node Group as EC2 Instance.\nCreate Amazon EKS Cluster.  At Cloud9 terminal, execute the command the below to create an Amazon EKS Cluster.  eksctl create cluster --name=fcj-storage-cluster --region=ap-southeast-1 --zones=ap-southeast-1a,ap-southeast-1b --without-nodegroup Then, verify the Cluster by command.  eksctl get cluster --region=ap-southeast-1 Enable kubectl to communicate with your cluster by adding a new context to the kubectl config file.  aws eks update-kubeconfig --region=ap-southeast-1 --name=fcj-storage-cluster Then, confirm communication with your cluster by running the following command.  kubectl get svc Note: The expected output is the appearance of ClusterIP service.\nCreate and associate IAM OIDC Provider for EKS Cluster. IAM OpenID Connect (OIDC) Provider help to use some Amazon EKS add-ons, or to enable individual Kubernetes workloads to have specific AWS Identity and Access Management (IAM) permissions.\n At Cloud9 terminal, execute the command the below to create and associate an OIDC Provider to your Amazon EKS Cluster.  eksctl utils associate-iam-oidc-provider --cluster=fcj-storage-cluster --region=ap-southeast-1 --approve To confirm created OIDC Provider, go to IAM. Navigate to Identity providers section. You will see there is a new Provider had been created.  Create Amazon EKS managed Node Group.  At Cloud9 terminal, execute the command the below to create managed Node Group and associate it to EKS Cluster.  eksctl create nodegroup --name=fcj-storage-nodegroup --cluster=fcj-storage-cluster --region=ap-southeast-1 --node-type=t3.medium --nodes=1  It will take you about 15 minutes to finish this process.   List existing nodes in cluster.\n  kubectl get nodes "
},
{
	"uri": "/4-eksstoragewithefs/4.4-dynamicprovision/",
	"title": "Dynamic Provisioning",
	"tags": [],
	"description": "",
	"content": "Create Manifest files  Create a directory for Dynamic Provisioning section.  mkdir -p EFS/dynamic-provision/kube-manifest\rls EFS/dynamic-provision Create file name storageclass.yaml inside dynamic-provision/kube-manifest.  touch EFS/dynamic-provision/kube-manifest/storageclass.yaml Open storageclass.yaml file, paste the below code. Replace with your EFS File System ID at fileSystemId field.  kind: StorageClass\rapiVersion: storage.k8s.io/v1\rmetadata:\rname: efs-sc\rprovisioner: efs.csi.aws.com\rparameters:\rprovisioningMode: efs-ap\rfileSystemId: fs-0db5cdf65fc6c821d\rdirectoryPerms: \u0026#34;700\u0026#34; Create file name pvc.yaml inside dynamic-provision/kube-manifest.  touch EFS/dynamic-provision/kube-manifest/pvc.yaml Open pvc.yaml file, paste the below code.  apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: efs-claim\rspec:\raccessModes:\r- ReadWriteMany\rstorageClassName: efs-sc\rresources:\rrequests:\rstorage: 5Gi Create file name pod.yaml inside dynamic-provision/kube-manifest.  touch EFS/dynamic-provision/kube-manifest/pod.yaml Open pod.yaml file, paste the below code.  apiVersion: v1\rkind: Pod\rmetadata:\rname: efs-app\rspec:\rcontainers:\r- name: app\rimage: centos\rcommand: [\u0026#34;/bin/sh\u0026#34;]\rargs: [\u0026#34;-c\u0026#34;, \u0026#34;while true; do echo $(date -u) \u0026gt;\u0026gt; /data/out; sleep 5; done\u0026#34;]\rvolumeMounts:\r- name: persistent-storage\rmountPath: /data\rvolumes:\r- name: persistent-storage\rpersistentVolumeClaim:\rclaimName: efs-claim Deploy resources  Execute below command to deploy resources.  kubectl apply -f EFS/dynamic-provision/kube-manifest List all created resources.  kubectl get sc,pvc,pod STATUS of Persistent Volume Claim (pvc) must be Bound, and of Pod must be Running.\n\r Go to EFS.\n  Click on your EFS File System.   Navigate to Access points tab.   You can see there is an Access point had been created since you specified parameters.provisioningMode is efs-ap on storageclass.yaml file.\nVerify the result.  Execute below command to verify that data is written onto EFS filesystem.  kubectl exec efs-app -- bash -c \u0026#34;cat data/out\u0026#34; There are a lot of data wrote to /data/out file. Let take note the name of three first data to compare later. In my case is Wed May 8 17:52:02 UTC 2024, Wed May 8 17:52:07 UTC 2024 and Wed May 8 17:52:12 UTC 2024.\nNow we will delete the current Pod. Then list all resources again to make sure that Pod was deleted.  kubectl delete pod efs-app\rkubectl get sc,pvc,pod Pod efs-app was deleted.\nCreate a new pod.  kubectl apply -f EFS/dynamic-provision/kube-manifest/pod.yaml\rkubectl get pod Verify the data on /data/out again.  kubectl exec efs-app -- bash -c \u0026#34;cat data/out\u0026#34; As you can see, the three first of data on /data/out now are still Wed May 8 17:52:02 UTC 2024, Wed May 8 17:52:07 UTC 2024 and Wed May 8 17:52:12 UTC 2024. The data is still kept while the pod is replaced with another one.\nClean up  Execute below command to delete all created resources.  kubectl delete -f EFS/dynamic-provision/kube-manifest 2. Verify that all resources had been deleted successfully.\nkubectl get sc,pvc,pod  Go to EFS.\n  Select your EFS File System.\n  Click on Delete.   Input your EFS File System ID to confirm.\n  Click on Confirm to delete.\n  "
},
{
	"uri": "/5-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Delete EKS Cluster  Execute the below command to delete EKS cluster.  eksctl delete cluster --name fcj-storage-cluster --region ap-southeast-1 It will take you about 20 minutes to delete.\n\rDelete Cloud9 Workspace  Go to Cloud9. Select FCJ-Workspace. Click on Delete.  Input Delete to confirm. Click on Delete.   Delete IAM Role  Delete IAM Role Name AmazonEKS_EFS_CSI_DriverRole.   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]