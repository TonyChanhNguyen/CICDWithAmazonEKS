[
{
	"uri": "/4-eksdbwithrds/4.1-createrdsdb/",
	"title": "Create Amazon RDS Database",
	"tags": [],
	"description": "",
	"content": "Create a Security Group for the Database Instance   Go to Security Group.\n  Click on Create security group.   Configure the Security Group settings:\n   Security Group name: Enter FCJ-Management-DB-SG. Description: Enter Security Group for DB instance. VPC: Select VPC of EKS Cluster named eksctl-fcj-db-cluster-cluster/VPC.   Add new Inbound rules with:   Type: MYSQL/Aurora. Source: Node\u0026rsquo;s security group.    Then, click on Create security group.   Security Group for the Database Instance was created successfully.   Create DB Subnet Group   Go to RDS Subnet Groups.\n  Click on Create DB subnet group.   In the Create DB subnet group interface:\n   Name: enter FCJ-Management-Subnet-Group. Description: enter Subnet Group for FCJ Management. VPC: Select the created VPC\u0026rsquo;s EKS Cluster. Availability Zones: Choose all AZs. Subnets: Choose all Subnets.    Then, click on Create.   DB Subnet Group was created successfully.   Create Amazon RDS Database Instance.   Go to RDS Database.\n  Click on Create database.   At Choose a database creation method field, select Standard create and MySQL at Engine type.   At Templates field, select Free tier.   Next, make detailed settings:\n   DB instance identifier: Enter fcj-management-db-instance. Master user: Enter admin Master password: Enter your choice (in the lab, enter 123Vodanhphai) Confirm password: Enter the password again.   Setting at Connectivity field:   Virtual private cloud (VPC): Select VPC\u0026rsquo;s EKS Cluster. DB subnet group: Select created DB subnet group on above step. Existing VPC security groups: Select created SG named FCJ-Management-DB-SG.    Scroll down to the end of page and click on Create database.   Creating Amazon RDS Database is processing.   It will take your about 10 minutes to create successfully.   Click on created Database and store its Endpoint to use later.   Test the connection to your Amazon RDS Database  At Cloud9 Terminal, create a ephemaral Pod for testing the connection to our Amazon RDS Database. Don\u0026rsquo;t forget to replace with your Amazon RDS Endpoint.  kubectl run test-connect-pod -it --rm --restart=Never --image=mysql:5.6 -- mysql -h \u0026lt;REPLACE-YOUR-RDS-ENDPOINT\u0026gt; -u root -p123Vodanhphai Show the schemas of database.  show schemas; Create a database fcjmgmt for your application.  CREATE DATABASE fcjmgmt;\rshow schemas; Create a new table name user inside database fcjmgmt.  use fcjmgmt;\rCREATE TABLE `user` ( `id` INT NOT NULL AUTO_INCREMENT , `first_name` VARCHAR(45) NOT NULL , `last_name` VARCHAR(45) NOT NULL , `email` VARCHAR(45) NOT NULL , `phone` VARCHAR(45) NOT NULL , `comments` TEXT NOT NULL , `status` VARCHAR(10) NOT NULL DEFAULT \u0026#39;active\u0026#39; , PRIMARY KEY (`id`)) ENGINE = InnoDB;\rshow tables; Enter exit to quit.   "
},
{
	"uri": "/2-prerequiste/2.1-createcloud9workspace/",
	"title": "Create Cloud9 workspace",
	"tags": [],
	"description": "",
	"content": "Create Cloud9 workspace   Go to Cloud9 at region ap-southeast-1.\n  Click on Create environment.   At Create environment page, input FCJ-Workspace at Name field.\n  Input Workspace for hands on workshop at Description field.\n  At Environment type field, keep default New EC2 instance.\n  At Instance type field, select Additional instance types.\n  At Additional instance types field, select t3.large.   Scroll down to the end of page and click on Create.   The workspace instance is being created.   It will take you about 2 minutes for the instance is created successfully.\n  After the instance is created successfully, click on Open to start your workspace.   "
},
{
	"uri": "/",
	"title": "Deploy database on Amazon EKS",
	"tags": [],
	"description": "",
	"content": "Deploy database on Amazon EKS Overview This workshop will provide a high level overview on how to integrate MySQL Database for application that run on Amazon EKS Cluster via Amazon Elastic Block Storage (EBS) and Amazon RDS services.\nContent  Introduction Prerequisites Deploy database on Amazon EKS via EBS Volume Deploy database on Amazon EKS via Amazon RDS Clean up resources  "
},
{
	"uri": "/3-eksdbwithebs/3.1-installcsidriver/",
	"title": "Install EBS CSI Driver",
	"tags": [],
	"description": "",
	"content": "The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver manages the lifecycle of Amazon EBS volumes as storage for the Kubernetes Volumes that you create. The Amazon EBS CSI driver makes Amazon EBS volumes for these types of Kubernetes volumes: generic ephemeral volumes and persistent volumes.\nCreate IAM Policy The Amazon EBS CSI plugin requires IAM permissions to make calls to AWS APIs on your behalf.\n  Go to IAM.\n  Select Policies section.\n  Click on Create policy.   At Policy editor field, select JSON tab.\n  Then, paste the below JSON.\n  {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:CreateSnapshot\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateVolume\u0026#34;,\r\u0026#34;ec2:DeleteSnapshot\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteVolume\u0026#34;,\r\u0026#34;ec2:DescribeInstances\u0026#34;,\r\u0026#34;ec2:DescribeSnapshots\u0026#34;,\r\u0026#34;ec2:DescribeTags\u0026#34;,\r\u0026#34;ec2:DescribeVolumes\u0026#34;,\r\u0026#34;ec2:DetachVolume\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r}  Then, click on Next.   Input Amazon_EBS_CSI_Driver as Policy name.\n  Input Policy for EC2 Instances to access Elastic Block Store as Description - optional.\n  Click on Create policy.   Get the IAM role Worker Nodes using  At Cloud9 Terminal, using below command to find out IAM role name that Worker Nodes using.  kubectl -n kube-system describe configmap aws-auth Verify the value of rolearn to find out the IAM Role name. Example: In this case, the IAM Role name is eksctl-fcj-db-cluster-nodegroup-fc-NodeInstanceRole-zwkug73VvB8W.\nAssociate policy to IAM Role   Go to IAM Role.\n  At Search feature, input the IAM Role name you had found out.\n  Click on it.   Click on Attach policies of Add permissions feature.   Search the policy name Amazon_EBS_CSI_Driver.\n  Select the result.\n  Click on Add permissions.   Install Amazon EBS CSI Driver  At Cloud9 Terminal, execute below command.  kubectl apply -k \u0026#34;github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=master\u0026#34; Verify the result. Make sure that there are ebs-csi pods are running at namespace kube-system.  kubectl get pods -n kube-system You had installed Amazon EBS CSI Driver successfully, move to next step to demonstrate how to use it to deploy EBS Storage for Amazon EKS Cluster. "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS). Kubernetes is an open-source system that automates the management, scaling, and deployment of containerized applications.\nAmazon RDS is an easy to manage relational database service optimized for total cost of ownership. It is simple to set up, operate, and scale with demand. Amazon RDS automates the undifferentiated database management tasks, such as provisioning, configuring, backups, and patching. Amazon RDS enables customers to create a new database in minutes, and offers flexibility to customize databases to meet their needs across 8 engines and 2 deployment options.\nAmazon Elastic Block Store (Amazon EBS) provides scalable, high-performance block storage resources that can be used with Amazon Elastic Compute Cloud (Amazon EC2) instances.\n"
},
{
	"uri": "/4-eksdbwithrds/4.2-deployapp/",
	"title": "Deploy application",
	"tags": [],
	"description": "",
	"content": "Create Manifest files.  Create a working directory for this section.  cd ..\rmkdir eks-with-rds\rls Copy 05-fcjmgmt-deployment.yaml and 06-fcjmgmt-svc.yaml on kube-manifest directory.  cp kube-manifest/05-fcjmgmt-deployment.yaml eks-with-rds\rcp kube-manifest/06-fcjmgmt-svc.yaml eks-with-rds\rls eks-with-rds To help your EKS Cluster connect to created Amazon RDS Database, we will use externalName service to create the connection.\nCreate file named 07-RDS-externalName-svc.yaml inside eks-with-rds.  touch eks-with-rds/07-RDS-externalName-svc.yaml\rls eks-with-rds OPen file 07-RDS-externalName-svc.yaml, paste the below code and save.  apiVersion: v1\rkind: Service\rmetadata:\rname: mysql-svc\rspec:\rtype: ExternalName\rexternalName: \u0026lt;REPLACE-WITH-YOUR-RDS-ENDPOINT\u0026gt; Deploy resources  At Cloud9 terminal, execute the below command to deploy resources.  kubectl apply -f eks-with-rds List all created resources.  kubectl get deploy,svc,pod There is a created Deployment named fcjmgmt-microservice, a created Service named fcjmgmt-restapp-service with Type is NodePort and PORT is 3231, a created Service named mysql-svc with Type is ExternalName and EXTERNAL-IP is RDS-ENDPOINT, and a created Pod ith STATUS is Running. 3. Get the Public IP Address of Node which placing application Pod.\nkubectl get node -o wide Access to application URL http://\u0026lt;REPLACE-WITH-NODE'S-EXTERNAL-IP\u0026gt;:31231.   Your application was deployed successfully.\n Let add some users to your application.   Now, let delete the current Deployment then re-create another one.\n  kubectl delete -f eks-with-rds/05-fcjmgmt-deployment.yaml List all running Deployment and Pod to make sure they are deleted totally.  kubectl get deploy,pod 8. Let re-create Deployment and Pod.\nkubectl apply -f eks-with-rds/05-fcjmgmt-deployment.yaml\rkubectl get deploy,pod Reload your website to see the result.   All of added user information are still kept while Deployment and Pod is rescheduled.\nMorever, you can also perform View, Edit or Delete operations with your user information.  Clean up  Delete all created resources.  kubectl delete -f eks-with-rds/ List all resources to make sure they are deleted totally.  kubectl get deploy,svc,pod All of them are deleted totally.\n Go to RDS Database.\n  Select your database.\n  Click on Action.\n  Click on Delete.   At Delete fcj-management-db-instance instance interface:\n   Uncheck Create final snapshot. Uncheck Retain automated backups. Check I acknowledge that upon instance deletion, automated backups, including system snapshots and point-in-time recovery, will no longer be available. Input delete me to confirm.  Then, click on Delete.  After about 10 minutes, your database will be deleted.   "
},
{
	"uri": "/3-eksdbwithebs/3.2-deploymysqlonebs/",
	"title": "Deploy MySQL database on EBS Volume",
	"tags": [],
	"description": "",
	"content": "In this section, we will utilize storage for EKS Cluster by creating EBS Volume dynamically. Then create a MySQL database on this storage to connect with application.\nCreate EBS Volume dynamically.  Create a folder to store all manifest files inside fcj-user-management.  cd ..\rmkdir kube-manifest\rcd kube-manifest Create a file named 01-storage-class.yaml.  touch 01-storage-class.yaml Open 01-storage-class.yaml file, paste the below code and save.  apiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata: name: ebs-sc\rprovisioner: ebs.csi.aws.com\rvolumeBindingMode: WaitForFirstConsumer Create a file named 02-pvc.yaml.  touch 02-pvc.yaml Open 02-pvc.yaml file, paste the below code and save.  apiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: ebs-mysql-pv-claim\rspec: accessModes:\r- ReadWriteOnce\rstorageClassName: ebs-sc\rresources: requests:\rstorage: 4Gi Create a file named 03-mysql-deployment.yaml.  touch 03-mysql-deployment.yaml Open 03-mysql-deployment.yaml file, paste the below code and save.  apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: mysql\rspec: replicas: 1\rselector:\rmatchLabels:\rapp: mysql\rstrategy:\rtype: Recreate template: metadata: labels: app: mysql\rspec: containers:\r- name: mysql\rimage: mysql:5.6\renv:\r- name: MYSQL_ROOT_PASSWORD\rvalue: 123Vodanhphai\rports:\r- containerPort: 3306\rname: mysql volumeMounts:\r- name: mysql-persistent-storage\rmountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage\rpersistentVolumeClaim:\rclaimName: ebs-mysql-pv-claim Create a file named 04-mysql-clusterip.yaml.  touch 04-mysql-clusterip.yaml Open 04-mysql-clusterip.yaml file, paste the below code and save.  apiVersion: v1\rkind: Service\rmetadata: name: mysql-svc\rspec:\rselector:\rapp: mysql ports: - port: 3306 clusterIP: None # This means we are going to use Pod IP Now, we will try to deploy MySQL Deployment to test MySQL Database.  kubectl apply -f . 11. List all created resources.\nkubectl get sc,pv,pvc,deploy,svc,pod There are a lot of created resources:\n A created Storage Class named ebs-sc. A created Persistent Volume (PV) with name created automatically, status is Bound and Capacity is 4Gi. A created Persistent Volume Claim (PVC) named ebs-mysql-pv-claim, status is Bound nd Capacity is 4Gi. A created service type ClusterIP named mysql-svc with Port is 3306. A created deployment named mysql with READY is 1/1 (mean there is 1 pod created successful with replicas is 1). A created pod named DeploymentName-xxxxxxx-xxxxx with STATUS is Running.  Now, let using an ephemeral Pod to test the connection to your MySQL database.  kubectl run test-mysql-ephemeral-pod -it --rm --image=mysql:5.6 -- mysql -h mysql-svc -p123Vodanhphai Your ephemeral Pod connected to MySQL database successfully. 13. Let list the schemas of MySQL database.\nshow schemas; Create a database fcjmgmt for your application.  CREATE DATABASE fcjmgmt; Let list the schemas of MySQL database again to see the created database fcjmgmt.  show schemas; Create a new table name user inside database fcjmgmt.  use fcjmgmt;\rCREATE TABLE `user` ( `id` INT NOT NULL AUTO_INCREMENT , `first_name` VARCHAR(45) NOT NULL , `last_name` VARCHAR(45) NOT NULL , `email` VARCHAR(45) NOT NULL , `phone` VARCHAR(45) NOT NULL , `comments` TEXT NOT NULL , `status` VARCHAR(10) NOT NULL DEFAULT \u0026#39;active\u0026#39; , PRIMARY KEY (`id`)) ENGINE = InnoDB;\rshow tables;  Enter exit to exit to your ephemeral pod. This ephemeral pod will be deleted after you exit to it.   Go to EBS Volume to verify there is a new created volume with Capacity is 4Gi.\n  "
},
{
	"uri": "/2-prerequiste/2.2-modifyiamrole/",
	"title": "Modify IAM role",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a IAM Role and assign it to workspace instance.\nCreate IAM role   Click IAM to navigate to IAM service.\n  Click on Role.\n  Click on Create role.   At Trusted entity type field, select AWS service.\n  At Service or use case field, select EC2.   Then, click on Next.   At Permissions policies field, select policy name AdministratorAccess.   Then, click on Next.   At Name, review, and create page, input eksworkspace-administrator at Role name field.   Then, scroll down to the end of page and click on Create role.   Assign role to workspace instance   At AWS Cloud9 interface, click on Manage EC2 instance.   You will see the created workspace instance. Then, click to select it.\n  Click on Action.\n  Click on Security.\n  Click on Modify IAM role.   Select the role name eksworkspace-administrator which was created at above steps.\n  Then, click on Update IAM role.   New IAM role was updated successfully.   Update Cloud9 configuration Cloud9 will manage IAM credentials automatically. This default configuration is currently not compatible with EKS authentication via IAM, we will need to disable this feature and use the IAM Role.\n\r  At AWS Cloud9 interface, click on AWS Cloud9.\n  Select Preferences.   At AWS Settings, disable AWS managed temporary credentials.   To ensure that temporary credentials are not saved in Cloud9, we will delete all existing credentials with the command below.\n  rm -vf ${HOME}/.aws/credentials "
},
{
	"uri": "/2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Overview To conduct the lab, we have to prepare the Cloud9 workspace instance and create the IAM role for the Cloud9 instance.\nContent  2.1 Create Cloud9 workspace 2.2 Modify IAM Role 2.3 Installation 2.4 Create Amazon EKS Cluster 2.5 Create application  "
},
{
	"uri": "/3-eksdbwithebs/3.3-deployappdeployment/",
	"title": "Deploy Application",
	"tags": [],
	"description": "",
	"content": "Now we will create manifest files to deploy application and its service.\nDeploy application  Create a file named 05-fcjmgmt-deployment.yaml.  touch 05-fcjmgmt-deployment.yaml Open 05-fcjmgmt-deployment.yaml file, paste the below code. Replace \u0026lt;YOUR-REPOSITORY-PATH:TAG\u0026gt; with your repository name and tag. In my case is firstcloudjourneypcr/fcj-management:v1.  apiVersion: apps/v1\rkind: Deployment metadata:\rname: fcjmgmt-microservice\rlabels:\rapp: fcjmgmt-restapp\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: fcjmgmt-restapp\rtemplate: metadata:\rlabels: app: fcjmgmt-restapp\rspec:\rcontainers:\r- name: fcjmgmt-restapp\rimage: \u0026lt;YOUR-REPOSITORY-PATH:TAG\u0026gt;\rports: - containerPort: 8080 env:\r- name: DB_HOST\rvalue: \u0026#34;mysql-svc\u0026#34; - name: DB_PORT\rvalue: \u0026#34;3306\u0026#34; - name: DB_NAME\rvalue: \u0026#34;fcjmgmt\u0026#34; - name: DB_USER\rvalue: \u0026#34;admin\u0026#34; - name: DB_PASS\rvalue: \u0026#34;123Vodanhphai\u0026#34; - name: PORT\rvalue: \u0026#34;8080\u0026#34; 3. Create a file named 06-fcjmgmt-svc.yaml.\ntouch 06-fcjmgmt-svc.yaml Open 06-fcjmgmt-svc.yaml file, paste the below code and save it.  apiVersion: v1\rkind: Service\rmetadata:\rname: fcjmgmt-restapp-service\rlabels: app: fcjmgmt-restapp\rspec:\rtype: NodePort\rselector:\rapp: fcjmgmt-restapp\rports: - port: 8080\rtargetPort: 8080\rnodePort: 31231 Let deploy your application.  kubectl apply -f . List all created resources.  kubectl get sc,pv,pvc,deploy,svc,pod There is a new created Service named fcjmgmt-restapp-service with TYPE is NodePort and PORT is 8080:31231/TCP, a new created Deployment named fcjmgmt-microservice and Pod with STATUS is Running.\nLet get the Public IP Address of your Node which placing Pod.  kubectl get node -o wide The Public IP Address is EXTERNAL-IP.\nLet access to your application to see the result with URL http://\u0026lt;EXTERNAL-IP\u0026gt;:31231.  Opps, your application can not be accessed. This since the Security Group of your Node which placing Pod still not opened for port 31231.\nGo to Security Group/ Click on resulted security group.   Click on Edit inbound rules.   Add a new rule with:\n Type is Custom TCP. Port range is 31231. Source is Anywhere-IPv4.    Click on Save rules.   Access again to application URL to see the result.   Your application had been deployed successfully.\nTest application   Click on Add New User.   Input all information and click on Submit.\n   Repeat above step to add more users.\n  Then, click on Home to back to your homepage.\n  Verify the result, there are some users had been add to your application.   Now we will delete application Pod and create new one verity see the data which you filled will be kept or not?\nAt Cloud9 terminal, delete application Pod.  kubectl delete -f 05-fcjmgmt-deployment.yaml List all resources again to make sure application pod and deployment deleted.  kubectl get pod,deployment They are deleted.\nLet create them again.  kubectl apply -f 05-fcjmgmt-deployment.yaml List all resources to make sure they are created.  kubectl get pod,deployment Access to application URL again to verify the result.   All of data are kept while Pod and Deployment rescheduled.\nClean up  At Cloud9 terminal, execute below command to delete all created resources.  kubectl delete -f . List all resources again to make sure that they are deleted totally.  kubectl get sc,pv,pvc,deploy,svc,pod All of them are deleted totally.\nGo to EBS Volume to verify the created EBS Volume deleted.  "
},
{
	"uri": "/3-eksdbwithebs/",
	"title": "Deploy database on Amazon EKS via EBS Volume",
	"tags": [],
	"description": "",
	"content": "Overview We are going to use EBS CSI Driver and use EBS Volumes for persistence storage to MySQL Database\nContent  3.1 Install EBS CSI Driver 3.2 Deploy MySQL database on EBS Volume 3.3 Deploy application  "
},
{
	"uri": "/2-prerequiste/2.3-installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will install necessary tools: awscli, kubectl and eksctl.\nUpgrade awscli  Copy and paste the command below into Terminal of Cloud9 Workspace to upgrade awscli.  sudo pip install --upgrade awscli \u0026amp;\u0026amp; hash -r Install kubectl  At Cloud9 Terminal, execute those command to install kubectl.   Update the instance packages.  sudo yum update  Install kubectl.  curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check version of kubectl.  kubectl version --client Install eksctl  At Cloud9 terminal, execute those command to install eksctl.   Download and extract the latest release of eksctl with the following command.  curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  Move the extracted binary to /usr/local/bin.  sudo mv /tmp/eksctl /usr/local/bin  Test that your installation was successful with the following command  eksctl version "
},
{
	"uri": "/2-prerequiste/2.4-createekscluster/",
	"title": "Create Amazon EKS Cluster",
	"tags": [],
	"description": "",
	"content": "In previous step, we installed necessary tools: awscli, kubectl and eksctl. Now, we will process to create an Amazon EKS Cluster with managed Node Group as EC2 Instance.\nCreate Amazon EKS Cluster.  At Cloud9 terminal, execute the command the below to create an Amazon EKS Cluster.  eksctl create cluster --name=fcj-db-cluster --region=ap-southeast-1 --zones=ap-southeast-1a,ap-southeast-1b --without-nodegroup Then, verify the Cluster by command.  eksctl get cluster --region=ap-southeast-1 Enable kubectl to communicate with your cluster by adding a new context to the kubectl config file.  aws eks update-kubeconfig --region=ap-southeast-1 --name=fcj-db-cluster Then, confirm communication with your cluster by running the following command.  kubectl get svc Note: The expected output is the appearance of ClusterIP service.\nCreate and associate IAM OIDC Provider for EKS Cluster. IAM OpenID Connect (OIDC) Provider help to use some Amazon EKS add-ons, or to enable individual Kubernetes workloads to have specific AWS Identity and Access Management (IAM) permissions.\n At Cloud9 terminal, execute the command the below to create and associate an OIDC Provider to your Amazon EKS Cluster.  eksctl utils associate-iam-oidc-provider --cluster=fcj-db-cluster --region=ap-southeast-1 --approve To confirm created OIDC Provider, go to IAM. Navigate to Identity providers section. You will see there is a new Provider had been created.  Create Amazon EKS managed Node Group.  At Cloud9 terminal, execute the command the below to create managed Node Group and associate it to EKS Cluster.  eksctl create nodegroup --name=fcj-db-nodegroup --cluster=fcj-db-cluster --region=ap-southeast-1 --node-type=t3.medium --nodes=1  It will take you about 15 minutes to finish this process.   List existing nodes in cluster.\n  kubectl get nodes "
},
{
	"uri": "/4-eksdbwithrds/",
	"title": "Deploy database on Amazon EKS via Amazon RDS",
	"tags": [],
	"description": "",
	"content": "Overview In this section, we will create an Amazon RDS instance. Then integrate it to EKS Cluster via ExternalName service.\nContent  4.1 Create Amazon RDS instance 4.2 Deploy application  "
},
{
	"uri": "/5-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Delete EKS Cluster  Execute below command to delete EKS Cluster.  eksctl delete cluster --name fcj-db-cluster --region ap-southeast-1 It will take you about 20 minutes to delete.\n\rDelete Cloud9 Workspace  Go to Cloud9. Select FCJ-Workspace. Click on Delete.  Input Delete to confirm. Click on Delete.   "
},
{
	"uri": "/2-prerequiste/2.5-createapplication/",
	"title": "Create Application",
	"tags": [],
	"description": "",
	"content": "In this section, we will clone a sample FCJ Management application.\nClone application.  Check the Git version of your workspace.  git version Upgrade Git to latest version.  sudo yum update git Create and move to working directory.  mkdir fcj-user-management\rcd fcj-user-management Clone application to your workspace.  git clone https://github.com/First-Cloud-Journey/000004-EC2.git List all resources of application.  ls\rls 000004-EC2 Rename folder 000004-EC2 to Application.  mv 000004-EC2 Application List all resources of application again.  ls\rls Application 8. Move to application directory and install dependencies of application on package.json.\ncd Application\rnpm install 9. Start application.\nnode app.js The application will get error Fail to connect to database, since there are no MySQL database created and provided for application to use.\nNow, we will containerize the application to Container Image by Docker. The MySQL Database will be provided in next sections.\nContainerize application  Check the version of Docker.  docker version Create a file name Dockerfile.  touch Dockerfile Open Dockerfile file, paste the code below and save.  # Use an official Node.js runtime as a parent image FROM node:13-alpine # Set the working directory in the container WORKDIR app # Copy package.json and package-lock.json to the working directory COPY package*.json ./ # Install dependencies RUN npm install # Copy the rest of the application code COPY . . # Expose the port the app runs on EXPOSE 5000 # Command to run the application CMD [\u0026#34;npm\u0026#34;,\u0026#34;start\u0026#34;] Create a file name .dockerignore to mitigate capacity for container image.  touch .dockerignore  Click on Settings symbol, select Show Hidden Files to see the hidden file .dockerignore.   Open .dockerignore file, paste the code below and save.\n  .git\rnode_modules\rDockerfile List all image in your workspace. Make sure there are no image named fcj-management since we will use this name for container image.  docker images Let build your application to container image.  docker build -t fcj-management:v1 . After this process finished successfully. Let list the images in your workspace again.  docker images The container image named fcj-management with tag v1 is created successfully from FCJ Management application. Now let use Docker to deploy your application from created container image for testing purpose.\nExecute the below command to deploy the application from created container image.  docker run -d --name testing-application -e PORT=8080 -p 8080:8080 fcj-management:v1 List the running process in your workspace.  docker ps Watching the logs of the application.  docker logs testing-application The errors are shown in log also Fail to connect to database. It means the container image is built successfully.\nLet delete running process.  docker stop testing-application\rdocker rm testing-application\rdocker ps -a Push container image to DockerHub In this step, we will push the created container image to DockerHub for Amazon EKS Cluster can pull to run Pod. So we will not deep dive how to use DockerHub, you can access to DockerHub Docs for detail.\n  Log in to DockerHub with your account.\n  Create a new repository named fcj-management.   Go to DockerHub Security to create new access token for Cloud9 workspace log in.\n  Click on New Access Token.   Input the Access Token Description as Access Token for FCJ Workshop.\n  Keep Access Permissions as default (with Read, Write, Delete permission).\n  Then, click on Generate.   Save your access token to use later.   Back to Cloud9 terminal, log in to DockerHub.\n  docker login -u firstcloudjourneypcr Enter your access token when be asked Password.   To push your container image to DockerHub repository. The repository of container image must be match with repository name on DockerHub.\nTo do that, we will tag the container image.  docker tag fcj-management:v1 firstcloudjourneypcr/fcj-management:v1 List all images on your workspace.  docker images There is a new duplicated image with repository named firstcloudjourneypcr/fcj-management, tag is v1.\nNow, let push the container image to DockerHub.  docker push firstcloudjourneypcr/fcj-management:v1 Go to DockerHub Repository firstcloudjourneypcr/fcj-management to see the result.  There is a new pushed image on your DockerHub repository with tag is v1.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]