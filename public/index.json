[
{
	"uri": "/",
	"title": "Create CICD Pipeline to deploy application on Amazon EKS Cluster",
	"tags": [],
	"description": "",
	"content": "Create CICD Pipeline to deploy application to Amazon EKS Overview Continuous Integration and Continuous Deployment (CI/CD) pipelines are essential for automating the deployment of applications. This workshop outlines the steps required to set up a CI/CD pipeline for deploying applications to an Amazon Elastic Kubernetes Service (EKS) cluster.\nContent  Introduction Prerequisites Create Amazon ECR Create STS Assume IAM Role for AWS CodeBuild Create CodePipeline Verify the result Cleanup resources  "
},
{
	"uri": "/2-prerequiste/2.1-createcloud9workspace/",
	"title": "Create Cloud9 workspace",
	"tags": [],
	"description": "",
	"content": "Create Cloud9 workspace   Go to Cloud9 at region ap-southeast-1.\n  Click on Create environment.   At Create environment page, input FCJ-Workspace at Name field.\n  Input Workspace for hands on workshop at Description field.\n  At Environment type field, keep default New EC2 instance.\n  At Instance type field, select Additional instance types.\n  At Additional instance types field, select t3.large.   Scroll down to the end of page and click on Create.   The workspace instance is being created.   It will take you about 2 minutes for the instance is created successfully.\n  After the instance is created successfully, click on Open to start your workspace.   "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that eliminates the need to install, operate, and maintain your own Kubernetes control plane on Amazon Web Services (AWS). Kubernetes is an open-source system that automates the management, scaling, and deployment of containerized applications.\nAmazon RDS is an easy to manage relational database service optimized for total cost of ownership. It is simple to set up, operate, and scale with demand. Amazon RDS automates the undifferentiated database management tasks, such as provisioning, configuring, backups, and patching. Amazon RDS enables customers to create a new database in minutes, and offers flexibility to customize databases to meet their needs across 8 engines and 2 deployment options.\nAmazon Elastic Block Store (Amazon EBS) provides scalable, high-performance block storage resources that can be used with Amazon Elastic Compute Cloud (Amazon EC2) instances.\n"
},
{
	"uri": "/2-prerequiste/2.2-modifyiamrole/",
	"title": "Modify IAM role",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a IAM Role and assign it to workspace instance.\nCreate IAM role   Click IAM to navigate to IAM service.\n  Click on Role.\n  Click on Create role.   At Trusted entity type field, select AWS service.\n  At Service or use case field, select EC2.   Then, click on Next.   At Permissions policies field, select policy name AdministratorAccess.   Then, click on Next.   At Name, review, and create page, input eksworkspace-administrator at Role name field.   Then, scroll down to the end of page and click on Create role.   Assign role to workspace instance   At AWS Cloud9 interface, click on Manage EC2 instance.   You will see the created workspace instance. Then, click to select it.\n  Click on Action.\n  Click on Security.\n  Click on Modify IAM role.   Select the role name eksworkspace-administrator which was created at above steps.\n  Then, click on Update IAM role.   New IAM role was updated successfully.   Update Cloud9 configuration Cloud9 will manage IAM credentials automatically. This default configuration is currently not compatible with EKS authentication via IAM, we will need to disable this feature and use the IAM Role.\n\r  At AWS Cloud9 interface, click on AWS Cloud9.\n  Select Preferences.   At AWS Settings, disable AWS managed temporary credentials.   To ensure that temporary credentials are not saved in Cloud9, we will delete all existing credentials with the command below.\n  rm -vf ${HOME}/.aws/credentials "
},
{
	"uri": "/2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Overview To conduct the lab, we have to prepare the Cloud9 workspace instance and create the IAM role for the Cloud9 instance.\nContent  2.1 Create Cloud9 workspace 2.2 Modify IAM Role 2.3 Installation 2.4 Create Amazon EKS Cluster 2.5 Create application  "
},
{
	"uri": "/3-interactwithecr/",
	"title": "Create Amazon ECR",
	"tags": [],
	"description": "",
	"content": "Create Amazon ECR  Go to Amazon ECR. Click on Get Started at Create a repository field.  At Repository name field, input fcj-cicd-repository.   Then, click on Create repository.   Save the URI to use later.   "
},
{
	"uri": "/2-prerequiste/2.3-installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will install necessary tools: awscli, kubectl and eksctl.\nUpgrade awscli  Copy and paste the command below into Terminal of Cloud9 Workspace to upgrade awscli.  sudo pip install --upgrade awscli \u0026amp;\u0026amp; hash -r Install kubectl  At Cloud9 Terminal, execute those command to install kubectl.   Update the instance packages.  sudo yum update  Install kubectl.  curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Check version of kubectl.  kubectl version --client Install eksctl  At Cloud9 terminal, execute those command to install eksctl.   Download and extract the latest release of eksctl with the following command.  curl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  Move the extracted binary to /usr/local/bin.  sudo mv /tmp/eksctl /usr/local/bin  Test that your installation was successful with the following command  eksctl version "
},
{
	"uri": "/2-prerequiste/2.4-createekscluster/",
	"title": "Create Amazon EKS Cluster",
	"tags": [],
	"description": "",
	"content": "In previous step, we installed necessary tools: awscli, kubectl and eksctl. Now, we will process to create an Amazon EKS Cluster with managed Node Group as EC2 Instance.\nCreate Amazon EKS Cluster.  At Cloud9 terminal, execute the command the below to create an Amazon EKS Cluster.  eksctl create cluster --name=fcj-cicd-cluster --region=ap-southeast-1 --zones=ap-southeast-1a,ap-southeast-1b --without-nodegroup Then, verify the Cluster by command.  eksctl get cluster --region=ap-southeast-1 Enable kubectl to communicate with your cluster by adding a new context to the kubectl config file.  aws eks update-kubeconfig --region=ap-southeast-1 --name=fcj-cicd-cluster Then, confirm communication with your cluster by running the following command.  kubectl get svc Note: The expected output is the appearance of ClusterIP service.\nCreate and associate IAM OIDC Provider for EKS Cluster. IAM OpenID Connect (OIDC) Provider help to use some Amazon EKS add-ons, or to enable individual Kubernetes workloads to have specific AWS Identity and Access Management (IAM) permissions.\n At Cloud9 terminal, execute the command the below to create and associate an OIDC Provider to your Amazon EKS Cluster.  eksctl utils associate-iam-oidc-provider --cluster=fcj-cicd-cluster --region=ap-southeast-1 --approve To confirm created OIDC Provider, go to IAM. Navigate to Identity providers section. You will see there is a new Provider had been created.  Create Amazon EKS managed Node Group.  At Cloud9 terminal, execute the command the below to create managed Node Group and associate it to EKS Cluster.  eksctl create nodegroup --name=fcj-cicd-nodegroup --cluster=fcj-cicd-cluster --region=ap-southeast-1 --node-type=t3.large --nodes=1  It will take you about 15 minutes to finish this process.   List existing nodes in cluster.\n  kubectl get nodes "
},
{
	"uri": "/4-createcodecommit/",
	"title": "Create CodeCommit Repository",
	"tags": [],
	"description": "",
	"content": "  Go to CodeCommit. Click on Create repository.   At Repository name field, input fcj-cicd-source.\n  At Description field, input Codecommit repository to store source code for FCJ Workshop.\n  Click on Create.   Click to Clone URL and select Clone HTTPS to get your repository URL.   The repository URL is copied automatically.\nAt Cloud9 Terminal, execute the below commands to clone the git repository from Code Commit to local repository. Replace with your copied repository URL.  git clone \u0026lt;Your-repository-URL\u0026gt; There is a new empty folder named fcj-cicd-source appeared in your workspace.\nMove all your created resources to fcj-cicd-source.  mv app fcj-cicd-source\rmv kube-manifest fcj-cicd-source\rmv Dockerfile fcj-cicd-source\rmv buildspec.yaml fcj-cicd-source\rls fcj-cicd-source Push your source code to CodeCommit Repository.  cd fcj-cicd-source\rgit add .\rgit commit -m \u0026#34;Add all source codes\u0026#34;\rgit push Reload your CodeCommit Repository to verify there are some resources pushed to.   "
},
{
	"uri": "/2-prerequiste/2.5-createapplication/",
	"title": "Create Application",
	"tags": [],
	"description": "",
	"content": "Create application  At the Cloud9 terminal, enter the command below to create a new directory for application.  mkdir app\rcd app Create a file named index.html inside app.  touch index.html\rls Open file index.html, paste the below code and save it.  \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;body style=\u0026#34;background-color:rgb(228, 250, 210);\u0026#34;\u0026gt;\r\u0026lt;h1\u0026gt;Welcome to First Cloud Journey - App Version - V1 \u0026lt;/h1\u0026gt;\r\u0026lt;h3\u0026gt;Create CICD Pipeline to deploy application on Amazon EKS Cluster Workshop\u0026lt;/h3\u0026gt;\r\u0026lt;p\u0026gt;Application Name: App1\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt; Create Dockerfile.  cd ..\rtouch Dockerfile Open Dockerfile, paste the below code and save it.  FROM nginx\rCOPY app /usr/share/nginx/html/app1 Create Manifest file  Create new directory to store Manifest files.  mkdir kube-manifest\rls Create new file 01-cicd-app-Deployment.yaml.  touch kube-manifest/01-cicd-app-Deployment.yaml Open file 01-cicd-app-Deployment.yaml, paste the below code and save it.  apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: fcj-cicd-deployment\rlabels:\rapp: fcj-cicd\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: fcj-cicd\rtemplate:\rmetadata:\rlabels:\rapp: fcj-cicd\rspec:\rcontainers:\r- name: fcj-cicd\rimage: CONTAINER_IMAGE\rports:\r- containerPort: 80 Create new file 02-cicd-app-NodePort-svc.yaml.  touch kube-manifest/02-cicd-app-NodePort-svc.yaml Open file 02-cicd-app-NodePort-svc.yaml, paste the below code and save it.  apiVersion: v1\rkind: Service\rmetadata:\rname: fcj-cicd-nodeport-service\rlabels:\rapp: fcj-cicd\rspec:\rtype: NodePort\rselector:\rapp: fcj-cicd\rports:\r- port: 80\rtargetPort: 80\rnodePort: 30000 Create buildspec.yaml  Create a file named buildspec.yaml.  touch buildspec.yaml Open file buildspec.yaml, paste the below code. At line 18, replace \u0026lt;REPLACE-WITH-YOUR-ACCOUNT-ID\u0026gt; with your Account ID then save it.  version: 0.2\rphases:\rinstall:\rcommands:\r- echo \u0026#34;Install Phase - Nothing to do using latest Amazon Linux Docker Image for CodeBuild which has all AWS Tools - https://github.com/aws/aws-codebuild-docker-images/blob/master/al2/x86_64/standard/3.0/Dockerfile\u0026#34;\rpre_build:\rcommands:\r# Docker Image Tag with Date Time \u0026amp; Code Buiild Resolved Source Version\r- TAG=\u0026#34;$(date +%Y-%m-%d.%H.%M.%S).$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | head -c 8)\u0026#34;\r# Update Image tag in our Kubernetes Deployment Manifest - echo \u0026#34;Update Image tag in kube-manifest...\u0026#34;\r- sed -i \u0026#39;s@CONTAINER_IMAGE@\u0026#39;\u0026#34;$REPOSITORY_URI:$TAG\u0026#34;\u0026#39;@\u0026#39; kube-manifest/01-cicd-app-Deployment.yaml\r# Verify AWS CLI Version - echo \u0026#34;Verify AWS CLI Version...\u0026#34;\r- aws --version\r# Login to ECR Registry for docker to push the image to ECR Repository\r- echo \u0026#34;Login in to Amazon ECR...\u0026#34;\r- aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin \u0026lt;REPLACE-WITH-YOUR-ACCOUNT-ID\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com\r# Update Kube config Home Directory\r- export KUBECONFIG=$HOME/.kube/config\rbuild:\rcommands:\r# Build Docker Image\r- echo \u0026#34;Build started on `date`\u0026#34;\r- echo \u0026#34;Building the Docker image...\u0026#34;\r- docker build --tag $REPOSITORY_URI:$TAG .\rpost_build:\rcommands:\r# Push Docker Image to ECR Repository\r- echo \u0026#34;Build completed on `date`\u0026#34;\r- echo \u0026#34;Pushing the Docker image to ECR Repository\u0026#34;\r- docker push $REPOSITORY_URI:$TAG\r- echo \u0026#34;Docker Image Push to ECR Completed - $REPOSITORY_URI:$TAG\u0026#34; # Extracting AWS Credential Information using STS Assume Role for kubectl\r- echo \u0026#34;Setting Environment Variables related to AWS CLI for Kube Config Setup\u0026#34; - CREDENTIALS=$(aws sts assume-role --role-arn $EKS_KUBECTL_ROLE_ARN --role-session-name codebuild-kubectl --duration-seconds 900)\r- export AWS_ACCESS_KEY_ID=\u0026#34;$(echo ${CREDENTIALS} | jq -r \u0026#39;.Credentials.AccessKeyId\u0026#39;)\u0026#34;\r- export AWS_SECRET_ACCESS_KEY=\u0026#34;$(echo ${CREDENTIALS} | jq -r \u0026#39;.Credentials.SecretAccessKey\u0026#39;)\u0026#34;\r- export AWS_SESSION_TOKEN=\u0026#34;$(echo ${CREDENTIALS} | jq -r \u0026#39;.Credentials.SessionToken\u0026#39;)\u0026#34;\r- export AWS_EXPIRATION=$(echo ${CREDENTIALS} | jq -r \u0026#39;.Credentials.Expiration\u0026#39;)\r# Setup kubectl with our EKS Cluster - echo \u0026#34;Update Kube Config\u0026#34; - aws eks update-kubeconfig --name $EKS_CLUSTER_NAME\r# Apply changes to our Application using kubectl\r- echo \u0026#34;Apply changes to kube manifest\u0026#34; - kubectl apply -f kube-manifest/\r- echo \u0026#34;Completed applying changes to Kubernetes Objects\u0026#34; # Create Artifacts which we can use if we want to continue our pipeline for other stages\r- printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;01-cicd-app-Deployment.yaml\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; $REPOSITORY_URI:$TAG \u0026gt; build.json\r# Additional Commands to view your credentials #- echo \u0026#34;Credentials Value is.. ${CREDENTIALS}\u0026#34; #- echo \u0026#34;AWS_ACCESS_KEY_ID... ${AWS_ACCESS_KEY_ID}\u0026#34; #- echo \u0026#34;AWS_SECRET_ACCESS_KEY... ${AWS_SECRET_ACCESS_KEY}\u0026#34; #- echo \u0026#34;AWS_SESSION_TOKEN... ${AWS_SESSION_TOKEN}\u0026#34; #- echo \u0026#34;AWS_EXPIRATION... $AWS_EXPIRATION\u0026#34; #- echo \u0026#34;EKS_CLUSTER_NAME... $EKS_CLUSTER_NAME\u0026#34; artifacts:\rfiles: - build.json - kube-manifests/* "
},
{
	"uri": "/5-createstsiamrole/",
	"title": "Create STS Assume IAM Role for AWS CodeBuild",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy changes to our Kubernetes manifests. This requires an AWS IAM role capable of interacting with the EKS cluster. In this step, we are going to create an IAM role and add an inline policy EKS:Describe that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl\nCreate IAM Role  At Cloud9 terminal, export your Account ID.  export ACCOUNT_ID=\u0026lt;REPLACE-WITH-YOUR-ACCOUNT-ID\u0026gt; Set Trust Policy.  TRUST=\u0026#34;{ \\\u0026#34;Version\\\u0026#34;: \\\u0026#34;2012-10-17\\\u0026#34;, \\\u0026#34;Statement\\\u0026#34;: [ { \\\u0026#34;Effect\\\u0026#34;: \\\u0026#34;Allow\\\u0026#34;, \\\u0026#34;Principal\\\u0026#34;: { \\\u0026#34;AWS\\\u0026#34;: \\\u0026#34;arn:aws:iam::${ACCOUNT_ID}:root\\\u0026#34; }, \\\u0026#34;Action\\\u0026#34;: \\\u0026#34;sts:AssumeRole\\\u0026#34; } ] }\u0026#34;\recho $TRUST Create IAM Role for CodeBuild to Interact with EKS.  aws iam create-role --role-name EksCodeBuildKubectlRole --assume-role-policy-document \u0026#34;$TRUST\u0026#34; --output text --query \u0026#39;Role.Arn\u0026#39; Define Inline Policy with eks Describe permission in a file iam-eks-describe-policy.  echo \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;eks:Describe*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] }\u0026#39; \u0026gt; /tmp/iam-eks-describe-policy Associate Inline Policy to our newly created IAM Role.  aws iam put-role-policy --role-name EksCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-eks-describe-policy Go to IAM Role to verify the created Role named EksCodeBuildKubectlRole.   Update EKS Cluster aws-auth ConfigMap with created Role.  Verify what is present in aws-auth configmap before change  kubectl get configmap aws-auth -o yaml -n kube-system Set ROLE value.  ROLE=\u0026#34; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026#34; Get current aws-auth configMap data and attach new role info to it.  kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026#34;/mapRoles: \\|/{print;print \\\u0026#34;$ROLE\\\u0026#34;;next}1\u0026#34; \u0026gt; /tmp/aws-auth-patch.yml Patch the aws-auth configmap with new role.  kubectl patch configmap/aws-auth -n kube-system --patch \u0026#34;$(cat /tmp/aws-auth-patch.yml)\u0026#34; Verify what is updated in aws-auth configmap after change  kubectl get configmap aws-auth -o yaml -n kube-system "
},
{
	"uri": "/6-createpipeline/",
	"title": "Create CodePipeline",
	"tags": [],
	"description": "",
	"content": " Go to CodePipeline. Click on Create pipeline.  At Pipeline name field, input the name of pipeline FCJ-CICD-EKS-CodePipeline.  Keep all another fields as default. Then, scroll down and click on Next.  At Add source stage interface, config the below parameters:   At Source provider, select AWS CodeCommit. At Repository name, select your created repository named fcj-cicd-source. At Branch name, select master branch. At Change detection options, choose the default (Amazon CloudWatch Events (recommended)). At Output artifact format, choose the defaut (CodePipeline default).   Then, click on Next.   At Add build stage interface, config the below parameters:\n   At Build provider, select AWS CodeBuild. At Region, select Asian Pacific (Singapore).  At Project name, click on Create project.   At Create build project interface, input FCJ-CICD-EKS-CodeBuildPrj as Project name and keep default at another fields.   Click to Additional configuration to expand more configurations.   Add more Environment variables as below:\n   REPOSITORY_URI = .dkr.ecr.ap-southeast-1.amazonaws.com/fcj-cicd-repository EKS_KUBECTL_ROLE_ARN = arn:aws:iam:::role/EksCodeBuildKubectlRole EKS_CLUSTER_NAME = fcj-cicd-cluster    At Build specifications field, select Use a buildspec file.   At Logs filed, input FCJ-CICD-EKS-CodeBuildPrj at Group Name.\n  Click on Continue to CodePipeline.   You should be received the notification that your CodeBuild project created successfully.\n  Then, click on Next.\n   At Add deploy stage interface, click on Skip deploy stage to skip this step.\n  Click Skip to confirm.   At Review interface, let review your configuration. Then, click on Create pipeline.\n  "
},
{
	"uri": "/7-verifyresult/",
	"title": "Verify the result",
	"tags": [],
	"description": "",
	"content": "You had created a Pipeline to deploy your application to Amazon EKS in previous step. Let verify your Pipeline and make the changes on your source code to see the result.\nVerify Pipeline.  Go to AWS CodePipeline, you will see there is a Pipeline named FCJ-CICD-EKS-CodePipeline but the Latest execution status is failed. Click on your Pipeline to investigate the reason.   The failing on Build state, click on View details for more information.   Scroll down to the end of log to see the failing by your EKS Cluster can not log in to ECR, since no identity-based policy allows the ecr:GetAuthorizationToken action on IAM Role codebuild-FCJ-CICD-EKS-CodeBuildPrj-service-role. Means CodeBuild not able to upload or push newly created Docker Image to ECR repository.   Now we need to attach which policy has ecr:GetAuthorizationToken action to IAM Role. Let go to IAM Policies and find the policy named AmazonEC2ContainerRegistryPowerUser, it has ecr:GetAuthorizationToken action so we can use it to attach to IAM Role.\n   Go to IAM Roles, find your IAM Role codebuild-FCJ-CICD-EKS-CodeBuildPrj-service-role. Click on it.   Let attach the policy named AmazonEC2ContainerRegistryPowerUser to your IAM Role.   After attaching policy to your IAM Role successfully, go back to your AWS CodePipeline. Click to Retry failed actions.    After Build step finished, the returned result is still fail. Let click on View details to see the reason.   The reason is CodeBuild do not have access to perform updates in EKS Cluster. Let create STS Assume Policy and Associate that to CodeBuild Role codebuild-FCJ-CICD-EKS-CodeBuildPrj-service-role.\n   Go to IAM Policies. Click on Create policy.   At Specify permissions interface, config as the below parameters:\n   At Service field, select STS. At Actions allowed field, select AssumeRole as write action. At Resources, add your ARN.  Then click on Next.  At Review and create interface, input the value:   Input fcj-codebuild-sts-assume-role as Policy name . Input CodeBuild to interact with EKS cluster to perform changes as Description - optional.   Then click on Create policy.   Go to IAM Roles and find your IAM Role named codebuild-FCJ-CICD-EKS-CodeBuildPrj-service-role.   Attach our created policy on above step to this IAM Role.   Back to your Pipeline and click on Retry stage to re-run Build Step.   After a few minutes, your Pipeline was built successfully.   Verify the result.  Back to your Cloud9 Terminal, list all created Deployments, Services and Pods on your EKS Cluster.  kubectl get deploy,svc,pod -o wide There are some created resources:\n A created Deployment named fcj-cicd-Deployment with Replicas is 1 and IMAGES is .dkr.ecr.ap-southeast-1.amazonaws.com/fcj-cicd-repository:(In my case IMAGES is 170074558790.dkr.ecr.ap-southeast-1.amazonaws.com/fcj-cicd-repository:2024-05-21.20.59.18.1303bea5). A created Service named fcj-cicd-nodeport-service with Type is NodePort and PORT is 80:30000/TCP. A created Pod named fcj-cicd-deployment-xxxxxxxxxx-xxxxx with STATUS is Running.   Go to ECR, click on your repository.   There is a created image on repository with Image tag match with on IMAGES of fcj-cicd-Deployment.   Get EXTERNAL-IP of your Node.\n  kubectl get node -o wide Access to your application URL http://\u0026lt;NODE's-EXTERNAL-IP\u0026gt;:30000.   You can not access to your application because port 30000 of your Node is still not opened.\n Go to your Security Group of your Node.   Click on Edit inbound rules.   At a new rule:\n   Type is Custom TCP. Port range is 30000. Source is Anywhere - IPv4.   Then, click on Save rules.   Let reload your application URL to verify the result.   Your application worked with version V1.\nMake changes to index.html.  Back to Cloud9 workspace, open file index.html inside fcj-cicd-source/app. Modify the code to version V2 and save it.  Push your code to AWS CodeCommit.  git add .\rgit commit -m \u0026#34;Modify to version V2\u0026#34;\rgit push  Go to your AWS CodePipeline to verify that your Pipeline was triggered to run the process.   Wait to the Pipeline finish with status is success. Reload your application URL again to verify the result.   Your application was deployed to version V2.\nCongratulations, you had deployed new version to your Amazon EKS Cluster with AWS CodePipeline successfully! "
},
{
	"uri": "/8-cleanup/",
	"title": "Cleanup resources",
	"tags": [],
	"description": "",
	"content": "Delete AWS CodePipeline.  Go to AWS CodePipeline, select your Pipeline. Click on Delete pipeline.  Input delete to confirm. Then, click on Delete.   Delete AWS CodeBuild Project.   Go to Build projects, select your project.\n  Click on Action and select Delete.   Input delete to confirm and then click on Delete.   Delete AWS CodeCommit Repository.   Go to AWS CodeCommit, select your repository.\n  Click on Delete repository.   Input delete to confirm and then click on Delete.   Delete Amazon ECR Repository.   Go to ECR, select your repository.\n  Click on Delete.   Input delete to confirm and then click on Delete.   Delete S3 Bucket.   Go to S3 and select bucket named codepipeline-ap-southeast-1-xxxxxxxxxxxx.\n  Click on Empty.   Input permanently delete and click on Empty.   Select your bucket again and click to Delete.\n  Input the name of bucket to confirm, then click to Delete bucket.   Delete Amazon EKS Cluster  Back to Cloud9 Terminal, delete your EKS Cluster.  eksctl delete cluster --name fcj-cicd-cluster --region ap-southeast-1 It will take you about 15 minutes to finish totally.  eksctl get cluster --region ap-southeast-1 Delete Cloud9 Workspace.  Go to Cloud9. Select FCJ-Workspace. Click on Delete.  Input Delete to confirm. Click on Delete.   "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]